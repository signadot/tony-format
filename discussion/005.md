<!-- Comment 005 - 2026-01-05T22:01:17+01:00 -->

-m ## Multi-Provider LLM API Landscape

For the dual-proxy approach, relevant to understand the API landscape:

### OpenAI-Compatible De Facto Standard

Many providers have adopted OpenAI's API format:

| OpenAI-compatible | Not OpenAI-compatible |
|-------------------|----------------------|
| OpenAI, Groq, Together AI, Fireworks, Mistral, Ollama, vLLM, xAI/Grok, Perplexity, DeepSeek | Anthropic, Google Gemini, Amazon Bedrock |

### Key Differences

| Aspect | Anthropic | OpenAI |
|--------|-----------|--------|
| Endpoint | `/v1/messages` | `/v1/chat/completions` |
| Tool calls | `tool_use` content block with `id` | `tool_calls[]` with `id` |
| Tool results | `tool_result` content block | `tool` role message |

### Implication

Two handlers in the LLM proxy cover ~90% of the market:
1. **Anthropic handler** - Claude models
2. **OpenAI-compatible handler** - GPT, Groq, Together, Mistral, Grok, local models, etc.

The correlation logic (tool_use_id â†’ conversation) works the same way for both, just extracted from different JSON paths. Internal storage format can be normalized, so downstream features (replay, what-if, judge) work regardless of provider.
