<!-- Comment 001 - 2025-12-26T04:11:06+01:00 -->

## Key Insight: Orchestrators are the Integration Point

LLM agents are stateless - they're pure functions of their context. The orchestrator holds the `messages` array, which IS the context sent to the model each turn.

If the orchestrator persists to tonyapi:
- Every turn's messages = full context the model sees
- Tool calls and results = what actions were taken
- Complete history via logd's diff storage

The model's "internal state" is derived fresh each turn from messages. Capturing messages captures reproducible state.

## Structured vs Unstructured Data in Agent State

| Structured | Unstructured |
|------------|--------------|
| Message roles, types | Message text content |
| Tool names, IDs | Tool descriptions |
| Tool input JSON | Freeform tool output |
| Conversation shape | Conversation substance |

Tony diffs handle both - structured parts diff precisely, text parts get line-level string diffs.

## A/B Testing Scenarios with Scopes

### 1. Model Version Comparison

Fork conversation at turn N, run same input with different models:

```tony
# Scope A (sonnet)
messages[5].content[0]:  # tool_use block
  name: read_file
  input: { path: "/src/main.go" }

# Scope B (opus)  
messages[5].content[0]:
  name: grep
  input: { pattern: "func main", path: "/src" }
```

Diff shows: different tool selection for same task.

### 2. System Prompt Iteration

Testing prompt changes on historical conversations:

```tony
# Scope A (original prompt)
messages[0].content: |
  You are a coding assistant.
  Always read files before editing.

# Scope B (revised prompt)
messages[0].content: !strdiff
  0: !replace "You are a coding assistant.\nAlways read files before editing."
  1: "You are a senior engineer.\nPrefer grep over reading entire files.\nExplain your reasoning."
```

Compare downstream behavior - does the new prompt change tool selection patterns?

### 3. Tool Implementation Changes

Same agent, different tool behavior:

```tony
# Scope A (old grep tool)
messages[3].content:  # tool_result
  tool_use_id: "xyz"
  content: "No matches found"

# Scope B (new grep with suggestions)
messages[3].content:
  tool_use_id: "xyz"  
  content: !strdiff
    0: "No matches found"
    1: "No matches found.\n\nDid you mean:\n- 'function' (3 matches)\n- 'func' (12 matches)"
```

Observe: does richer tool output change agent's next action?

### 4. Context Window Strategies

Fork at turn 50, compare summarization approaches:

```tony
# Scope A (full context)
messages: # 50 messages, 80k tokens

# Scope B (summarized)
messages[0..10]:  # first 10 preserved
messages[11]:     # summary injection
  role: user
  content: "[Previous conversation summary: user asked to refactor auth module...]"
messages[12..15]: # recent 4 preserved
```

Compare task completion rates.

### 5. Replay with Intervention

"What if I had given different feedback at turn 12?"

```tony
# Scope A (original)
messages[12].content: "looks good, continue"

# Scope B (alternative)  
messages[12].content: "wait, check for error handling first"
```

Fork from there, observe divergence.

### What Scopes Enable

- Branch from any commit
- Isolated mutation
- Diff between branches (not just against baseline)
- Merge insights back (e.g., "prompt B is better, adopt it")

The observability isn't just logging - it's comparative analysis across variations.
