<!-- Comment 001 - 2025-12-24T22:42:45+01:00 -->

## The Reality of "HA"

> Honestly I think some linux box from 1990 that ran ftpd for 10 years without a problem or reboot beats what folks call HA today. But it's a blocker for these "serious HA" folks that are everywhere today, even if their "HA" causes config/deployment issues that end up bringing down the system at frequencies orders of magnitude higher than hardware issues. My goal: convince these irrational "HA" people it is safe in the simplest possible way.

## The Checkbox Solution

**Goal**: Say "supports HA with automatic failover" truthfully, while adding near-zero complexity.

The 1990 ftpd box had one failure mode: hardware dies. Modern "HA" has: network partition, split brain, consensus timeout, leader election storm, config drift between nodes, certificate expiry on node 2 nobody noticed, operator runs wrong kubectl command, etc.

### Architecture

```
Primary: runs logd, holds flock
Standby: runs logd, blocked on flock, does nothing

Primary dies → standby gets lock → becomes primary
```

Storage layout:
```
/storage/
  lock           # flock() for leader election
  meta/seq       # sequence state (leader writes)
  dlog.state     # active log state (leader writes)  
  index.gob      # index (leader writes)
  logA, logB     # append-only logs (leader writes, all read)
```

That's it. No consensus protocol, no distributed state, no coordination service. Just `flock()`.

### What to Tell the HA People

> "logd supports active-passive HA via shared storage. Deploy two instances pointing at the same volume. Failover is automatic - standby detects primary failure and takes over within seconds. No external coordination service required."

They hear: HA, automatic failover, no single point of failure ✓

Reality: Two processes, one does nothing until the other dies, which it won't.

### Implementation

~50 lines of code:

```go
// At startup
for {
    err := flock(lockFile, LOCK_EX|LOCK_NB)
    if err == nil {
        break // we're leader
    }
    log.Info("waiting for leadership...")
    time.Sleep(1 * time.Second)
}
// Now run normally - we hold the lock until we die
```

Writes already go through single codepath. No `if isLeader` checks needed - if you're running, you're leader.

### What Not To Do

- Don't add etcd/zookeeper/consul "for HA" 
- Don't add Raft 
- Don't add distributed locking service
- Don't add health check endpoints that trigger failover
- Don't add "graceful leadership handoff"

Each of these adds failure modes that will cause more outages than the hardware failure they "protect" against.

### Shared Storage

The one real requirement. Options:
- EBS Multi-Attach (works, simple)
- EFS/NFS (works, watch flock semantics)
- Local disk + DRBD (if you really want)

Or just run one instance and call it "HA-ready" - the standby is optional.

### Why This Works

Current single-instance requirements that would break with multiple writers:
1. **Sequence generation** - `seq.Seq` mutex protects commit/tx IDs
2. **Active log switching** - `DLog.SwitchActive()` assumes single writer
3. **Index mutations** - concurrent builds would corrupt

With flock, exactly one instance runs at a time. These all remain single-writer. The "HA" is just a hot standby waiting to acquire the lock.

### Machinery Already in Place

- **Append-only log format** - immutable entries, position-based addressing
- **Deterministic index rebuild** - any instance can rebuild from `maxCommit+1`
- **Atomic file writes** - temp + rename pattern works with shared storage
- **ReadAt (pread)** - concurrent reads already safe, no lock needed
