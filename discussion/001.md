<!-- Comment 001 - 2025-12-27T01:12:31+01:00 -->

## Deep Sketch: Proxy-Based Agent Infrastructure with Controllers

### Overview

Enterprise-friendly agent observability via proxies that intercept LLM and tool traffic. Proxies are implemented as docd controllers, enabling the full tonyapi architecture from day one.

### Architecture

```
Customer's Agent (unchanged)
        │
        ├── ANTHROPIC_BASE_URL=proxy:8080
        │         │
        │         ▼
        │   ┌─────────────────────────────────────────────────────┐
        │   │              LLM Proxy Controller                    │
        │   │                                                      │
        │   │  HTTP :8080 ◀── Agent (Anthropic API)               │
        │   │       │                                              │
        │   │       ├──▶ Forward to api.anthropic.com             │
        │   │       │                                              │
        │   │       └──▶ PATCH to logd                            │
        │   │             /conversations/{id}/turns                │
        │   │                                                      │
        │   │  MOUNT /conversations ──▶ docd                      │
        │   └─────────────────────────────────────────────────────┘
        │
        └── MCP endpoints via proxy
                  │
                  ▼
            ┌─────────────────────────────────────────────────────┐
            │              MCP Proxy Controller                    │
            │                                                      │
            │  stdio/HTTP ◀── Agent (MCP protocol)                │
            │       │                                              │
            │       ├──▶ Forward to real MCP servers              │
            │       │                                              │
            │       └──▶ PATCH to logd                            │
            │             /conversations/{id}/tools                │
            │                                                      │
            │  MOUNT /tools ──▶ docd                              │
            └─────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│                           docd                                   │
│                                                                  │
│   Routes MATCH/PATCH/WATCH to controllers by mount point        │
│   Composes schema from controller contributions                  │
│   Coordinates multi-mount transactions                           │
│                                                                  │
│   Mounts:                                                        │
│     /conversations  ◀── LLM Proxy Controller                    │
│     /tools          ◀── MCP Proxy Controller                    │
│     /agents         ◀── Agent Controller (config, prompts)      │
│     /experiments    ◀── Experiment Controller (scopes, A/B)     │
│     /replay         ◀── Replay Controller (mock, shadow)        │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
                              │
              all controllers communicate directly with
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│                           logd                                   │
│                                                                  │
│   /conversations/{id}/turns[]     ← captured LLM traffic        │
│   /conversations/{id}/meta        ← model, tokens, timing       │
│   /agents/{id}/config             ← prompts, params             │
│   /experiments/{id}/variants      ← scoped branches             │
│                                                                  │
│   Features:                                                      │
│     • Diff storage (audit trail)                                │
│     • Scopes (COW for experiments)                              │
│     • Watches (live debugging)                                  │
│     • Multi-participant transactions                             │
└─────────────────────────────────────────────────────────────────┘
```

### Controller Definitions

#### 1. LLM Proxy Controller

Mounts at `/conversations`. Speaks Anthropic/OpenAI API on HTTP, patches captured traffic to logd.

```go
type LLMProxyController struct {
    docd     *docd.Client   // MOUNT registration
    logd     *logd.Client   // storage
    upstream string         // api.anthropic.com
    corr     *Correlator    // tool_use_id → conversation mapping
}

func (c *LLMProxyController) Start() error {
    // Register with docd
    err := c.docd.Mount("/conversations", &MountConfig{
        Schema: conversationSchema,
    })
    if err != nil {
        return err
    }

    // Serve Anthropic API
    return http.ListenAndServe(":8080", c)
}

func (c *LLMProxyController) ServeHTTP(w http.ResponseWriter, r *http.Request) {
    // Extract/generate conversation ID
    convID := c.extractConvID(r)

    // Capture request
    reqBody := captureRequest(r)

    // Forward to Anthropic
    resp := c.forward(r)
    respBody := captureResponse(resp)

    // Register pending tool_use IDs for correlation
    for _, block := range respBody.Content {
        if block.Type == "tool_use" {
            c.corr.Register(block.ToolUse.ID, convID)
        }
    }

    // Patch to logd
    turn := &Turn{
        Request:   reqBody,
        Response:  respBody,
        Timestamp: time.Now(),
        Latency:   elapsed,
        Tokens:    respBody.Usage,
    }
    c.logd.Patch(ctx, fmt.Sprintf("/conversations/%s/turns", convID), turn)

    // Return to agent
    copyResponse(w, resp)
}

// docd routes MATCH here
func (c *LLMProxyController) HandleMatch(path string, query *ir.Node) (*ir.Node, error) {
    return c.logd.Match(ctx, path, query)
}
```

#### 2. MCP Proxy Controller

Mounts at `/tools`. Wraps MCP servers (stdio or HTTP), captures tool calls.

```go
type MCPProxyController struct {
    docd     *docd.Client
    logd     *logd.Client
    corr     *Correlator   // shared with LLM Proxy
    wrapped  string        // path to real MCP server
}

// Stdio wrapper mode
func (c *MCPProxyController) RunStdio() error {
    // Mount with docd
    c.docd.Mount("/tools", &MountConfig{Schema: toolSchema})

    // Spawn real MCP server
    cmd := exec.Command(c.wrapped)
    realIn, _ := cmd.StdinPipe()
    realOut, _ := cmd.StdoutPipe()
    cmd.Start()

    // Intercept bidirectionally
    go c.interceptIn(os.Stdin, realIn)   // agent → real server
    go c.interceptOut(realOut, os.Stdout) // real server → agent

    return cmd.Wait()
}

func (c *MCPProxyController) interceptIn(from io.Reader, to io.Writer) {
    dec := json.NewDecoder(from)
    enc := json.NewEncoder(to)

    for {
        var msg mcp.Message
        dec.Decode(&msg)

        if msg.Method == "tools/call" {
            // Look up conversation from tool_use_id
            convID, ok := c.corr.Lookup(msg.Params.ToolUseID)
            if ok {
                c.logd.Patch(ctx, fmt.Sprintf("/conversations/%s/tools", convID),
                    &ToolCall{
                        ID:        msg.Params.ToolUseID,
                        Name:      msg.Params.Name,
                        Input:     msg.Params.Arguments,
                        Timestamp: time.Now(),
                    })
            }
        }

        enc.Encode(msg) // forward
    }
}
```

#### 3. Agent Controller

Mounts at `/agents`. Manages agent configurations (prompts, model settings).

```go
type AgentController struct {
    docd *docd.Client
    logd *logd.Client
}

func (c *AgentController) Start() error {
    return c.docd.Mount("/agents", &MountConfig{
        Schema: agentSchema,
    })
}

// Agents are just config documents
// Controller handles validation, defaults
func (c *AgentController) HandlePatch(path string, patch *ir.Node) (int64, error) {
    // Validate agent config
    if err := validateAgentConfig(patch); err != nil {
        return 0, err
    }

    // Apply to logd
    return c.logd.Patch(ctx, path, patch)
}
```

#### 4. Experiment Controller

Mounts at `/experiments`. Manages scopes for A/B testing.

```go
type ExperimentController struct {
    docd *docd.Client
    logd *logd.Client
}

func (c *ExperimentController) Start() error {
    return c.docd.Mount("/experiments", &MountConfig{
        Schema: experimentSchema,
    })
}

// Create experiment = create logd scope
func (c *ExperimentController) HandlePatch(path string, patch *ir.Node) (int64, error) {
    // Extract experiment definition
    exp := parseExperiment(patch)

    // Create scope in logd for each variant
    for _, variant := range exp.Variants {
        scopeID := fmt.Sprintf("exp-%s-%s", exp.ID, variant.Name)
        c.logd.CreateScope(ctx, scopeID, exp.BaselineConv)

        // Apply variant changes (e.g., different prompt)
        if variant.PromptOverride != "" {
            c.logd.PatchInScope(ctx, scopeID,
                fmt.Sprintf("/conversations/%s/meta/systemPrompt", exp.BaselineConv),
                variant.PromptOverride)
        }
    }

    return c.logd.Patch(ctx, path, patch)
}
```

#### 5. Replay Controller

Mounts at `/replay`. Orchestrates replay/mock modes.

```go
type ReplayController struct {
    docd      *docd.Client
    logd      *logd.Client
    llmProxy  *LLMProxyController  // to set mode
    mcpProxy  *MCPProxyController
}

type ReplayMode string
const (
    ReplayExact     ReplayMode = "exact"      // full mock, no calls
    ReplayMockTools ReplayMode = "mock-tools" // LLM live, tools mocked
    ReplayShadow    ReplayMode = "shadow"     // live + compare to stored
)

func (c *ReplayController) HandlePatch(path string, patch *ir.Node) (int64, error) {
    req := parseReplayRequest(patch)

    // Load conversation to replay
    conv := c.logd.Match(ctx, fmt.Sprintf("/conversations/%s", req.ConversationID), nil)

    switch req.Mode {
    case ReplayExact:
        // Return stored responses without calling anything
        c.llmProxy.SetMode(ModeReplay, conv)
        c.mcpProxy.SetMode(ModeReplay, conv)

    case ReplayMockTools:
        // LLM calls are live, tool calls return stored results
        c.llmProxy.SetMode(ModePassthrough, nil)
        c.mcpProxy.SetMode(ModeReplay, conv)

    case ReplayShadow:
        // Live calls, but compare to stored and log diffs
        c.llmProxy.SetMode(ModeShadow, conv)
        c.mcpProxy.SetMode(ModeShadow, conv)
    }

    // Create replay record
    return c.logd.Patch(ctx, path, &ReplayRecord{
        ID:             uuid.New().String(),
        ConversationID: req.ConversationID,
        Mode:           req.Mode,
        StartedAt:      time.Now(),
    })
}
```

### Correlation Strategy

Tool calls are correlated to conversations via `tool_use_id`:

```go
type Correlator struct {
    mu      sync.RWMutex
    pending map[string]string  // tool_use_id → conversation_id
    ttl     time.Duration
}

// LLM Proxy calls this when it sees tool_use in response
func (c *Correlator) Register(toolUseID, convID string) {
    c.mu.Lock()
    defer c.mu.Unlock()
    c.pending[toolUseID] = convID

    // Auto-expire after TTL
    time.AfterFunc(c.ttl, func() {
        c.mu.Lock()
        delete(c.pending, toolUseID)
        c.mu.Unlock()
    })
}

// MCP Proxy calls this to find conversation for a tool call
func (c *Correlator) Lookup(toolUseID string) (string, bool) {
    c.mu.RLock()
    defer c.mu.RUnlock()
    convID, ok := c.pending[toolUseID]
    return convID, ok
}
```

For multi-instance deployment, correlator state lives in logd:

```tony
/correlator/pending:
  toolu_01ABC: conv-123
  toolu_02DEF: conv-456
```

### Schema

```tony
define:
  Conversation:
    id: .string
    agentId: .string
    createdAt: .string
    meta: .ConversationMeta
    turns: !key(seq) .array(.Turn)

  ConversationMeta:
    model: .string
    systemPrompt: .string

  Turn:
    seq: !logd-auto-id .int
    timestamp: .string
    request: .LLMRequest
    response: .LLMResponse
    tools: !key(id) .array(.ToolExecution)
    latency: .int
    tokens: .TokenUsage

  LLMRequest:
    messages: .array(.Message)
    tools: .array(.ToolDef)

  LLMResponse:
    content: .array(.ContentBlock)
    stopReason: .string

  ToolExecution:
    id: .string
    name: .string
    input: .any
    output: .any
    error: .string?
    latency: .int
    timestamp: .string

  TokenUsage:
    input: .int
    output: .int

  Agent:
    id: .string
    name: .string
    model: .string
    systemPrompt: .string
    tools: .array(.string)

  Experiment:
    id: .string
    baselineConv: .string
    variants: .array(.Variant)

  Variant:
    name: .string
    promptOverride: .string?
    modelOverride: .string?

accept:
  conversations: !key(id) .array(.Conversation)
  agents: !key(id) .array(.Agent)
  experiments: !key(id) .array(.Experiment)
  replay: !key(id) .array(.ReplayRecord)
```

### Customer Integration

```bash
# Point LLM SDK at proxy
export ANTHROPIC_BASE_URL=https://llm.tonyapi.customer.internal:8080

# Wrap MCP servers
# Before: npx @modelcontextprotocol/server-filesystem /path
# After:
tonyapi-mcp-proxy npx @modelcontextprotocol/server-filesystem /path

# Or via config
# mcp.json
{
  "servers": {
    "filesystem": {
      "command": "tonyapi-mcp-proxy",
      "args": ["npx", "@modelcontextprotocol/server-filesystem", "/path"]
    }
  }
}
```

### CLI

```bash
# List conversations
tonyapi match /conversations

# View conversation
tonyapi match /conversations/conv-123

# View specific turn
tonyapi match /conversations/conv-123/turns/5

# Diff between turns
tonyapi diff /conversations/conv-123/turns/4 /conversations/conv-123/turns/5

# Replay with mocked tools
tonyapi patch /replay '{
  conversationId: conv-123
  mode: mock-tools
}'

# Create A/B experiment
tonyapi patch /experiments '{
  id: prompt-test-1
  baselineConv: conv-123
  variants:
  - name: verbose-prompt
    promptOverride: "You are a senior compliance analyst..."
  - name: concise-prompt
    promptOverride: "Compliance checker. Be brief."
}'

# Compare experiment results
tonyapi diff /conversations/conv-123 --scope=exp-prompt-test-1-verbose-prompt \
             /conversations/conv-123 --scope=exp-prompt-test-1-concise-prompt
```

### Implementation Order

1. **Phase 1: Core proxies (no docd yet)**
   - LLM Proxy → logd directly
   - MCP Proxy → logd directly
   - Correlator (in-memory)
   - Schema in logd

2. **Phase 2: Add docd**
   - docd with MOUNT support
   - Proxies register as controllers
   - MATCH routing through docd

3. **Phase 3: Additional controllers**
   - Agent Controller
   - Experiment Controller
   - Replay Controller

4. **Phase 4: Enterprise features**
   - Multi-instance correlator (logd-backed)
   - Auth/policy at docd layer
   - UI dashboard

### Path to Model B (Orchestrator)

With this architecture, Model B (tonyapi runs the agent) is an extension:

```
Phase 1-4: Proxies capture traffic from customer's agent
    ↓
Model B: Add Orchestrator Controller
    - Reads agent config from /agents/{id}
    - Makes LLM calls directly (not proxied)
    - Executes tools directly
    - Customer sends input, we run the agent
```

The controllers infrastructure supports both models.

