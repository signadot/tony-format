<!-- Comment 001 - 2026-01-04T22:12:00+01:00 -->

## Design Summary (from implementation discussion)

### What Gets Removed

1. **Patches before cutoff** - historical reads become approximate (snap to nearest snapshot)
2. **Superseded scope snapshots** - keep only most recent per active scope
3. **All data for deleted/inactive scopes** - no resurrection needed
4. **Completed/aborted schema migration entries** - only active schema matters

### Logarithmic Snapshot Retention

Space grows O(log T) over time via exponential bucketing:

```
Within cutoff:     all patches (accurate historical reads)
1-2x cutoff:       1 snapshot per interval
2-4x cutoff:       1 snapshot per 2 intervals  
4-8x cutoff:       1 snapshot per 4 intervals
...
```

Each tier has fixed slots (e.g., 8). When a tier fills, oldest graduates to next tier (sparser), others deleted.

**Snapshot grouping**: baseline + scope snapshots at same commit are treated as a unit - kept or deleted together.

### Pinned Snapshots

**Active schema snapshot is pinned** - never deleted until new migration completes. Schema remains coupled with snapshots to avoid mid-flight re-indexing complexity during migrations. When migration completes, pin moves to new snapshot; old one becomes subject to tier policy.

### Non-Blocking Compaction

**Flow:**
1. Write compacted log to temp file (doesn't affect reads)
2. Atomic swap: temp → inactive log, old inactive → pending delete
3. Swap index atomically with file
4. Wait for active readers to finish (refcount-based)
5. Delete old file

**Reader Reference Counting** - both log files have refcount tracking active readers:

```go
type Storage struct {
    logReaders [2]atomic.Int64  // refcount per log file
}
```

Reads acquire before I/O, release after completion.

**Grace Period with Timeout** - after swap, wait for refcount → 0 with timeout. Lingering readers error out cleanly (file deleted) - no silent corruption, no indefinite blocking.

**No index versioning needed (for now)** - watch replay uses short-lived reads (returns slice, no held iterator). If incremental/streaming replay added later, add version checks then.

### Configuration

```go
type CompactionConfig struct {
    Cutoff       time.Duration  // accurate history window (e.g., 1h)
    BaseInterval time.Duration  // tier 0 interval (e.g., 1h)
    SlotsPerTier int            // slots per tier (e.g., 8)
    Multiplier   int            // tier interval multiplier (e.g., 2)
    GracePeriod  time.Duration  // reader drain timeout (e.g., 5s)
}
```

### Files

**New:**
- `storage/compaction_policy.go` - tier policy, survivor selection
- `storage/compaction.go` - Compact() implementation

**Modified:**
- `server/config.go` - add CompactionConfig
- `storage/storage.go` - add Compact(), reader refcounts
- `storage/internal/dlog/dlog.go` - iteration for compaction reads
- `storage/index/index.go` - rebuild after compaction
